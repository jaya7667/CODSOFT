{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import re\n","import string\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import LancasterStemmer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from preprocessing.text import Tokenizer\n","from tensorflow.python.keras.models import Sequential\n","from tensorflow.python.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n","from tensorflow.python.keras.callbacks import EarlyStopping"]},{"cell_type":"markdown","metadata":{},"source":["Data Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_path='C:\\Users\\pinki\\OneDrive\\Documents\\GitHub\\CODSOFT\\Task 2\\train_data.txt'\n","train_data=pd.read_csv( train_path , sep=':::',engine='python',names=['Title','Genre','Description'])\n","train_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_path='C:\\Users\\pinki\\OneDrive\\Documents\\GitHub\\CODSOFT\\Task 2\\test_data.txt'\n","test_data=pd.read_csv( test_path , sep=':::',engine='python',names=['ID', 'Title','Description'])\n","test_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data.isnull().sum()"]},{"cell_type":"markdown","metadata":{},"source":["Data Visualizations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(12,8))\n","counts = train_data.Genre.value_counts()\n","sns.barplot(x=counts, y=counts.index, orient='h')  \n","plt.xlabel('Genre')\n","plt.ylabel('Count')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(12,8))\n","counts = train_data.Genre.value_counts()\n","sns.barplot(x=counts.index, y=counts, color='blue')\n","plt.xlabel('Genre' ,fontsize=14, fontweight='bold')\n","plt.ylabel('Count', fontsize=14, fontweight='bold')\n","plt.title('Distribution of Genres', fontsize=16, fontweight='bold')\n","plt.xticks(rotation=90, fontsize=14, fontweight='bold');"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data['length']=train_data['Description'].apply(len)\n","train_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(8, 7))\n","\n","sns.histplot(data=train_data, x='length', bins=20, kde=True, color='blue')\n","\n","plt.xlabel('Length', fontsize=14, fontweight='bold')\n","plt.ylabel('Frequency', fontsize=14, fontweight='bold')\n","plt.title('Distribution of Lengths', fontsize=16, fontweight='bold')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Data Cleaning and Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["stemmer = LancasterStemmer()\n","stop_words = set(stopwords.words('english'))\n","\n","def clean_text(text):\n","   \n","    text = text.lower()                                  # lower-case all characters\n","    text = re.sub('-',' ',text.lower())   # replace `word-word` as `word word`\n","    text = re.sub(f'[{string.digits}]',' ',text)  # remove digits\n","    text = ' '.join([stemmer.stem(word) for word in text.split() if word not in stop_words])  # remove stopwords and stem other words\n","    text =  re.sub(r'@\\S+', '',text)                     # remove twitter handles\n","    text =  re.sub(r'http\\S+', '',text)                  # remove urls\n","    text =  re.sub(r'pic.\\S+', '',text) \n","    text =  re.sub(r\"[^a-zA-Z+']\", ' ',text)             # only keeps characters\n","    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text+' ')      # keep words with length>1 only\n","    text = \"\".join([i for i in text if i not in string.punctuation])\n","    words = nltk.tokenize.word_tokenize(text,language=\"english\", preserve_line=True)\n","    stopwords = nltk.corpus.stopwords.words('english')   # remove stopwords\n","    text = \" \".join([i for i in words if i not in stopwords and len(i)>2])\n","    text= re.sub(\"\\s[\\s]+\", \" \",text).strip()            # remove repeated/leading/trailing spaces\n","    return re.sub(f'[{re.escape(string.punctuation)}]','',text) # remove punctuations\n","\n","\n","# Test your cleaning function\n","input_text = \"Certainly you get a dramatic boost from hello bye the the hi -iv iem-k q934*2yee !*3 2e38\"\n","print(f'Original text: {input_text}')\n","print(f'Cleaned text: {clean_text(input_text)}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data['Text_cleaning'] = train_data.Description.apply(clean_text)\n","test_data['Text_cleaning'] = test_data.Description.apply(clean_text)\n","\n","train_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data['length_Text_cleaning']=train_data['Text_cleaning'].apply(len)\n","train_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create histograms for text length before and after cleaning\n","plt.figure(figsize=(12, 6))\n","\n","# Original text length distribution\n","plt.subplot(1, 2, 1)\n","original_lengths = train_data['Description'].apply(len)\n","plt.hist(original_lengths, bins=range(0, max(original_lengths) + 100, 100), color='blue', alpha=0.7)\n","plt.title('Original Text Length')\n","plt.xlabel('Text Length')\n","plt.ylabel('Frequency')\n","\n","# Cleaned text length distribution\n","plt.subplot(1, 2, 2)\n","cleaned_lengths = train_data['Text_cleaning'].apply(len)\n","plt.hist(cleaned_lengths, bins=range(0, max(cleaned_lengths) + 100, 100), color='green', alpha=0.7)\n","plt.title('Cleaned Text Length')\n","plt.xlabel('Text Length')\n","plt.ylabel('Frequency')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["(train_data['length_Text_cleaning']>2000).value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Remove extremely long descriptions: outliers\n","print('Dataframe size (before removal): ',len(train_data))\n","filt=train_data['length_Text_cleaning']>2000\n","train_data.drop(train_data[filt].index,axis=0,inplace=True)     # filter rows having cleaned description length > 2000\n","print('Dataframe size (after removal): ',len(train_data))\n","print(f'Removed rows: {filt.sum()}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(12,5))\n","sns.barplot(x='Genre',y='length_Text_cleaning',data=train_data)  # from 600ish to 350ish -> significant reduction in length\n","plt.xticks(rotation=60)\n","plt.show()\n","plt.figure(figsize=(20,5))\n","sns.boxplot(x=train_data['length_Text_cleaning'].values,hue='Genre',data=train_data)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Text Tokenozation and Vectorization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_words = 50000\n","max_len = 250\n","tokenizer = Tokenizer(num_words=num_words, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n","tokenizer.fit_on_texts(train_data['Text_cleaning'].values)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_path='C:\\Users\\pinki\\OneDrive\\Documents\\GitHub\\CODSOFT\\Task 2\\test_data_soluttion.txt'\n","test_data_solution=pd.read_csv( test_path , sep=':::',engine='python',names=['ID','Title','Genre','Description'])\n","test_data_solution.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X = tokenizer.texts_to_sequences(train_data['Text_cleaning'].values)\n","X = pad_sequences(X, maxlen=max_len)\n","y = pd.get_dummies(train_data['Genre']).values\n","\n","X_test = tokenizer.texts_to_sequences(test_data['Text_cleaning'].values)\n","X_test = pad_sequences(X_test, maxlen=max_len)\n","y_test = pd.get_dummies(test_data_solution['Genre']).values"]},{"cell_type":"markdown","metadata":{},"source":["Building the LSTM model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["EMBEDDING_DIM = 100\n","model = Sequential()\n","model.add(Embedding(num_words, EMBEDDING_DIM, input_length=X.shape[1]))\n","model.add(SpatialDropout1D(0.2))\n","model.add(LSTM(100, dropout=0.1, recurrent_dropout=0.2))\n","model.add(Dense(27, activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{},"source":["Training the model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["my_callbacks  = [EarlyStopping(monitor='val_loss',\n","                              min_delta=0,\n","                              patience=2,\n","                              mode='auto')]\n","history = model.fit(X, y, epochs=6, batch_size=32,validation_data=(X_test,y_test), callbacks=my_callbacks)"]},{"cell_type":"markdown","metadata":{},"source":["Plotting Accuracy and Loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(12, 4))\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['accuracy'], label='Train Accuracy')\n","plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","plt.legend()\n","plt.title('Accuracy')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['loss'], label='Train Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.legend()\n","plt.title('Loss')\n","\n","plt.tight_layout()\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":2}
